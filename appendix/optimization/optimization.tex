\chapter{Optimization}
\label{chapter:optimization}

\FloatBarrier
\section{Optimization}

\subsection{Introduction}

Optimization is the process of finding the best solution for a problem that may also need to satisfy a number of imposed constraints.  In terms of this work, there are several tasks where optimization plays a key role.  The \acrshort{dft} code used, PWscf, employs the \acrlong{bfgs} algorithm to relax structures to give the optimum volume and optimal structure of an arrangement of atoms.  During the fitting of the variety options of equation of state to the energy-volume points, the \acrlong{lma} is used.  Finally, a genetic algorithm and simulated annealing algorithm are coupled with \acrshort{lma} to locate the global optimal parameters for interatomic potential functions.


\subsection{Continuous and Discrete Optimization}

Problems that may be solved using discrete optimisation have, as the name suggests, discrete solutions.  An example would be the travelling salesman problem, where the aim is to minimise the distance travelled by the salesman between cities.  An example of the solution may be city E to A to B to C to D.  There's a set of possible solutions to the problem.  If the problem was changed such that any point within a 1 mile radius of the centre of each city is a valid solution, and the aim is to find the optimal points to travel between within the area of each city, it would become a continuous problem.

Fitting interatomic potentials to data is an example of continuous optimisation.  The Morse potential takes three parameters plus the separation r between the two atoms.  If the parameters are being optimised such that Morse potential matches the experimental forces (assuming they could be measured experimentally) between two atoms for a set of separation values, the choice of parameters isn't discrete; they are taken from a continuous range of real numbers.



\subsection{Constrained and Unconstrained Optimization}

There are constraints that may need to be considered, and constraints that apply to the fitting of interatomic potentials are as follows:

\begin{itemize}
\item continuous well behaved function
\item continuous first-derivative, smooth change in force wrt r
\item positive electron density
\item smooth cutoff at some $r = r_{cut}$
\item repulsion (ZBL/exponential) as r approaches 0
\item continuous second-derivative
\item enough parameters coupled with a good choice of functions so the potential is flexible enough to replicate all the experimental data well
\item a small enough range of parameters to keep the parameter space small enough to search in a reasonable amount of time
\item maintain physical elegance
\item ignore physical elegance
\end{itemize}

Certain constraints listed will battle one-another, while others may be dropped, such as restricting to positive electron-densities if the choice to ignore physical elegance is selected. 


\FloatBarrier
\subsection{Global and Local Optimization}

An example to discuss the difference between local and global optimization will be given in a very literal sense.  I am located near Birmingham, and I wish to find the highest point local to me.  Using a modern smartphone and map I could quickly find the nearest hill to my current position; this would be the local maxima.  I would, however, need to go up and down many peaks and troughs, scouring the entire surface of the Earth, until I found the global maxima, somewhere amongst the many peaks and troughs in the Himalayas. 

When optimising the parameters of a function, it is relatively easy to find a local extreme that would give me the optimum parameters locally.  It is a much harder task to find the global optimum, especially if very little, or nothing at all, is known about the function for which the parameters are being optimised.



\subsection{Global Optimization Algorithms}


\subsubsection{Simulated Annealing}
\label{section:optsimulatedannealing}

A large amount of computing power is required today to solve many problems in Physics.  The optimum arrangement of Iron atoms, at various concentrations and temperatures is one such example, but nature knows the correct structure it should take because it has access to the rule book.  It makes perfect sense to look to nature when designing optimisation algorithms.

Simulated annealing mimics the way a solid cools, with its atoms settling into the optimum relaxed positions as time passes.  Initially, the solid is hot and the atoms are able to take non-optimal positions, but as the solid cools the atoms take their optimal places.  The heating allows the atoms to move from non-optimal positions in the initial configuration.

The algorithm was originally developed to improve upon the Metropolis algorithm, solving the issue of becoming stuck in non-global minima.  Motivation for the algorithm included applying it to the optimum placement of chips on a circuit board\cite{simanneal}, the travelling salesman (and similar) problem as well as finding the optimum arrangement of atoms.


\begin{lstlisting}[style=sPseudo,caption={Pseudocode for the simulated annealing algorithm},label={listing:simulatedannealing}]
# Calculate the starting rss
rss = get_rss(p)

# Loop (n times, until a threshold is met, time runs out etc)
while(loop)
{  
  p_new = vary(p)            # Make a new set of parameters
  rss_new = get_rss(p_new)   # Get rss for these parameters

  # Always accept BETTER
  # Sometimes accept WORSE (depends on temperature and how bad it is)
  if(rss_new < rss or rand(0.0,1.0) < exp((rss-rss_new) / temp))
  {
    p = p_new
    rss = rss_new
  }
}    
\end{lstlisting}


The simulated annealing algorithm takes in a starting set of parameters.  These are varied and, if the new parameters give a better solution to the problem, the parameters are updated.  However, there is a chance that a worse set of parameters are used, and this depends on how bad they are and the current value of the temperature in the algorithm (listing \ref{listing:simulatedannealing}).  The algorithm used is this work is available in the appendix (section \ref{section:simulatedannealing}).


\subsubsection{Genetic Algorithm}
\label{section:optgeneticalgorithm}

It is unclear whether life started on this planet, or was brought to this planet, and this idea only removes the problem of how life as we are able to understand it was conceived to a different time and place.  However it started, there is evidence of life on this planet approximately 3.5 billion years ago.  Prokaryotes were the simple single cell organisms that inhabited our planet for billions of these.  Their cell does not have a nucleus and the genetic information is contained in RNA and DNA within the cell.

Approximately 1.5 billion years ago, life made a leap forward in complexity and evolved into the first Eukaryotic cells.  These are larger and more complex than Prokaryotes, and the genetic material is stored within a nucleus.  Through processes of inheritance, variation, natural selection and the vast expanse of time, more complex organisms developed.

A set of solutions are bred with one another, and where there is an improvement the new variation is reinserted into the gene pool.  There is a danger that the population can become a set of clones, so a large pool of sets of parameters is required, fresh parameters proposed every so often as well as a mechanism to prevent clones appearing.

As simulated annealing takes inspiration from cooling solids, genetic algorithms take inspiration from evolution.  The algorithm used is this work is available in the appendix (section \ref{section:geneticalgorithm}).


\subsubsection{Basin-Hopping}

Lennard-Jones clusters are test systems of atoms models using the potential of the same name.  Clusters of varying sizes have an optimal configuration with the minimum amount of energy.  An algorithm was developed in the 1990s to improve upon existing global search algorithms and was applied to clusters of between 2 and 110 atoms\cite{basinhopping}.  The resulting basin-hopping algorithm allowed Wales and Doye to find three previously unknown minima.

\begin{equation}
\begin{split}
\tilde{E}(\vec{X}) = min \left(E(\vec{x})\right)
\end{split}
\label{eq:bashinhoppingenergy}
\end{equation}

The objective of the algorithm was to solve eq. \ref{eq:bashinhoppingenergy} where the vector $\vec{X}$ is the coordinates of the atoms.  A number of iterations, set by the user, are stepped through.  The co-ordinates are randomly perturbed before the local minima is determined by using a local optimizer such as \acrshort{bfgs}, \acrshort{lmbfgs} or \acrshort{cg}\cite{basinhoppingscipy}.  Similar to \acrshort{sa}, the minimized parameters are either accepted or discarded dependent upon how good the new parameters are.



\subsection{Local Optimization Algorithms}


\subsubsection{\acrfull{qp}}

The method of \acrlong{qp}, or quadratic optimization, is the task of minimising the matrix equation eq. \ref{eq:quadraticprogram} subject to imposed conditions\cite{nocedalwright1}.

\begin{equation}
\begin{split}
min x  \text{   } q(\vec{x}) = \frac{1}{2} \vec{x}^{T} \vec{G} \vec{x} + \vec{x}^{T} c \\
\text{subject to } a_i^T x = b_i, i \in \varepsilon  \\
a_i^T x  \geq b_i,   i \in I 
\end{split}
\label{eq:quadraticprogram}
\end{equation}

This is the method used by Bonny et al. in their potential fitting program and will be discussed in section \ref{section:fittingprograms}.


\subsubsection{Gradient Descent}

The gradient descent method it a reliable way of searching for a local minimum.  It requires only the first derivative coupled with a line search to move closer to the minimum point.  Close to the minimum, it is slower than the Newton-Gauss and LMA, but these require the computation of the Jacobian to estimate the Hessian matrix, so these are more computationally intensive.  A pseudocode for the algorithm is available in the appendix (section \ref{section:geneticalgorithm}).


\subsubsection{\acrlong{ng}}

The \acrlong{ng} method is an algorithm that finds the local minimum of a function by approximating the Hessian matrix with the Jacobian and its transpose (eq. \ref{eq:NewtonGauss}).  By estimating the Hessian, the algorithm only requires the computation of the first order derivatives with respect to each parameter, at each point pair x, y.  It is possible to use the function and the analytical derivative, but as this work requires the calculation of functions that have no simple analytical form, the first derivative will be approximated within the algorithm.

\begin{equation}
  \begin{split}
    \vec{J}^{T} \vec{J} \vec{p} = \vec{J}^{T} \vec{r} \\
    \vec{r} = \vec{y} - f(\vec{x}, \vec{p})
  \end{split}
  \label{eq:NewtonGauss}
\end{equation}

Close to a minima, the \acrshort{ng} method quickly moves to the optimum point.  Depending on the starting point, it may converge only to a local minimum, and not the global, and it may also be unstable, not converging at all.




\subsubsection{Levenberg Marquardt}

The \acrshort{lma}  is a \acrshort{ng} type algorithm that includes the addition of a dampening term that helps to increase the robustness of the algorithm.  The particular algorithm here uses a number of other schemes to improve the overall effectiveness of the algorithm.  The equation at the center of the algorithm is similar to that of \acrshort{ng} (eq. \ref{eq:LevenbergMarquardt}).

\begin{equation}
  \begin{split}
    \left( \vec{J}^{T} \vec{W} \vec{J} + \lambda diag\left( \vec{J}^{T} \vec{W} \vec{J} \right) \right) \vec{p} = {\left(\vec{J}^{T} \vec{W} \vec{r} \right)} \\
    \vec{r} = \vec{y} - f(\vec{x}, \vec{p})
  \end{split}
  \label{eq:LevenbergMarquardt}
\end{equation}

The starting value for lambda is calculated as a function of the estimate of the Hessian and a cutoff for lambda is introduced to remove dampening altogether.  A delayed gratification scheme has also been introduced to increase lambda by 50 percent if the trial solution is worse, and to decrease lambda by a factor of 5 if the trial solution is better. Finally, a diagonal weighting matrix has been included to allow certain parameters to be preferential during their optimisation.




\subsubsection{Conjugate Gradient}

In the method of steepest decent an initial value is selected and the gradient at that point is computed.  This gives a search direction to look in and, following a line search, a new point is selected where the gradient is orthogonal to the search gradient.  The process is iterative and at each new point the search gradient is replaced with the new orthogonal gradient (eq. \ref{eq:steepestdescent}\cite{conjugategradient}).

\begin{equation}
  \begin{split}
    r_{(i)} = b - A x_{(i)} \\
    \alpha = \frac{r^{T}_{i} r_{(i)}}{r_{(i)}^{T} A r_{(i)}} \\
    x_{(i+1)} = x_{(i)} + \alpha_{(i)} r_{(i)}
  \end{split}
  \label{eq:steepestdescent}
\end{equation}

The conjugate gradient method is similar

\begin{equation}
  \begin{split}
  d_{(0)} = r_{(0)} = b - A x_{(0)} \\  
  \alpha_{i} = \frac{r^{T}_{(i)} r_{(i)}}{d^{T}_{(i)} A d_{(i)}} \\
  x_{(i+1)} = x_{(i)} + \alpha_{(i)} d_{(i)} \\
  r_{(i+1)} = r_{(i)} - \alpha_{(i)} A d_{(i)} \\
  \beta_{(i+1)} = \frac{r^{T}_{(i+1)} r_{(i+1)}} {r^{T}_{(i)} r_{(i)}} \\
  d_{(i + 1)} = r_{(i + 1)} + \beta_{(i+1)} d_{(i)}\\
  \end{split}
  \label{eq:conjugategradient}
\end{equation}


\subsubsection{\acrfull{bfgs}}
